class EMA(nn.Module):
    def __init__(self, channels, groups=4):
        super(EMA, self).__init__()
        assert channels % groups == 0, "Channels must be divisible by groups"
        self.groups = groups
        self.gc = channels // groups  # channels per group

        self.fusion_conv = nn.Conv2d(self.gc, self.gc, kernel_size=1, bias=False)
        self.conv_3x3 = nn.Conv2d(self.gc, self.gc, kernel_size=3, padding=1, bias=False)
        self.conv_3x3_csl = nn.Conv2d(self.gc, self.gc, kernel_size=3, padding=1, bias=False)
        self.norm = nn.GroupNorm(groups, channels)
        self.sigmoid = nn.Sigmoid()

    def check_nan(tensor, name):
        if torch.isnan(tensor).any():
            print(f"‚ùå NaN detected in {name}")
            print(f"  Min: {tensor.min().item() if torch.isfinite(tensor).any() else 'NaN'}")
            print(f"  Max: {tensor.max().item() if torch.isfinite(tensor).any() else 'NaN'}")
            raise ValueError(f"NaN found in {name}")
    
    def forward(self, x):
        B, C, H, W = x.shape
        G, gc = self.groups, self.gc
    
        check_nan(x, "input x")
    
        x_grouped = x.view(B, G, gc, H, W).reshape(B * G, gc, H, W)
        check_nan(x_grouped, "x_grouped")
    
        # 1. Directional Attention
        x_pool = F.adaptive_avg_pool2d(x_grouped, (1, W))
        y_pool = F.adaptive_avg_pool2d(x_grouped, (H, 1))
        y_pool_t = y_pool.permute(0, 1, 3, 2)
        check_nan(x_pool, "x_pool")
        check_nan(y_pool_t, "y_pool_t")
    
        fusion = torch.cat([x_pool, y_pool_t], dim=3)
        fusion = self.fusion_conv(fusion)
        check_nan(fusion, "fusion")
    
        weight_x = self.sigmoid(fusion[:, :, 0, :W].unsqueeze(2))
        weight_y = self.sigmoid(fusion[:, :, 0, W:].unsqueeze(3))
        check_nan(weight_x, "weight_x")
        check_nan(weight_y, "weight_y")
    
        directional_attn = torch.matmul(weight_y, weight_x)
        check_nan(directional_attn, "directional_attn")
    
        x_reweighted = x_grouped * directional_attn
        check_nan(x_reweighted, "x_reweighted")
    
        # 2. Cross-Spatial Learning (Path 1)
        normed_input = x_reweighted.view(B, G, gc, H, W).view(B, C, H, W)
        normed = self.norm(normed_input).view(B * G, gc, H, W)
        check_nan(normed, "normed")
    
        avg_x1 = F.adaptive_avg_pool2d(normed, (H, 1))
        avg_y1 = F.adaptive_avg_pool2d(normed, (1, W))
        avg_x1 = torch.clamp(avg_x1, -50, 50)
        avg_y1 = torch.clamp(avg_y1, -50, 50)
    
        soft_x1 = F.softmax(avg_x1, dim=2).mean(1)
        soft_y1 = F.softmax(avg_y1, dim=3).mean(1)
        check_nan(soft_x1, "soft_x1")
        check_nan(soft_y1, "soft_y1")
    
        attn_map1 = torch.bmm(soft_x1, soft_y1).view(B, G, 1, H, W)
        check_nan(attn_map1, "attn_map1")
    
        # 3. Cross-Spatial Learning (Path 2)
        csl_feat = self.conv_3x3_csl(x_grouped)
        check_nan(csl_feat, "csl_feat")
    
        avg_x2 = F.adaptive_avg_pool2d(csl_feat, (H, 1))
        avg_y2 = F.adaptive_avg_pool2d(csl_feat, (1, W))
        avg_x2 = torch.clamp(avg_x2, -50, 50)
        avg_y2 = torch.clamp(avg_y2, -50, 50)
    
        soft_x2 = F.softmax(avg_x2, dim=2).mean(1)
        soft_y2 = F.softmax(avg_y2, dim=3).mean(1)
        check_nan(soft_x2, "soft_x2")
        check_nan(soft_y2, "soft_y2")
    
        attn_map2 = torch.bmm(soft_x2, soft_y2).view(B, G, 1, H, W)
        check_nan(attn_map2, "attn_map2")
    
        # Combine and output
        attn_sigmoid = self.sigmoid(attn_map1 + attn_map2)
        check_nan(attn_sigmoid, "attn_sigmoid")
    
        residual = attn_sigmoid + x_grouped.view(B, G, gc, H, W)
        check_nan(residual, "residual")
    
        out = x_reweighted.view(B, G, gc, H, W) * residual
        out = out.view(B, C, H, W)
        check_nan(out, "output out")
    
        return out


def check_nan(tensor, name):
        if torch.isnan(tensor).any():
            print(f" NaN detected in {name}")
            print(f"  Min: {tensor.min().item() if torch.isfinite(tensor).any() else 'NaN'}")
            print(f"  Max: {tensor.max().item() if torch.isfinite(tensor).any() else 'NaN'}")
            raise ValueError(f"NaN found in {name}")

class EMA(nn.Module):
    def __init__(self, channels, groups=4):
        super(EMA, self).__init__()
        assert channels % groups == 0, "Channels must be divisible by groups"
        self.groups = groups
        self.gc = channels // groups  # channels per group

        self.fusion_conv = nn.Conv2d(self.gc, self.gc, kernel_size=1, bias=False)
        self.conv_3x3 = nn.Conv2d(self.gc, self.gc, kernel_size=3, padding=1, bias=False)
        self.conv_3x3_csl = nn.Conv2d(self.gc, self.gc, kernel_size=3, padding=1, bias=False)
        self.norm = nn.GroupNorm(groups, channels)
        self.sigmoid = nn.Sigmoid()
    

    def forward(self, x):
        B, C, H, W = x.shape
        G, gc = self.groups, self.gc
    
        check_nan(x, "input x")
    
        x_grouped = x.view(B, G, gc, H, W).reshape(B * G, gc, H, W)
        check_nan(x_grouped, "x_grouped")
    
        # 1. Directional Attention
        x_pool = F.adaptive_avg_pool2d(x_grouped, (1, W))
        y_pool = F.adaptive_avg_pool2d(x_grouped, (H, 1))
        y_pool_t = y_pool.permute(0, 1, 3, 2)
        check_nan(x_pool, "x_pool")
        check_nan(y_pool_t, "y_pool_t")
    
        fusion = torch.cat([x_pool, y_pool_t], dim=3)
        fusion = self.fusion_conv(fusion)
        check_nan(fusion, "fusion")
    
        weight_x = self.sigmoid(fusion[:, :, 0, :W].unsqueeze(2))
        weight_y = self.sigmoid(fusion[:, :, 0, W:].unsqueeze(3))
        check_nan(weight_x, "weight_x")
        check_nan(weight_y, "weight_y")
    
        directional_attn = torch.matmul(weight_y, weight_x)
        check_nan(directional_attn, "directional_attn")
    
        x_reweighted = x_grouped * directional_attn
        check_nan(x_reweighted, "x_reweighted")
    
        # 2. Cross-Spatial Learning (Path 1)
        normed_input = x_reweighted.view(B, G, gc, H, W).view(B, C, H, W)
        normed = self.norm(normed_input).view(B * G, gc, H, W)
        check_nan(normed, "normed")
    
        avg_x1 = F.adaptive_avg_pool2d(normed, (H, 1))
        avg_y1 = F.adaptive_avg_pool2d(normed, (1, W))
        avg_x1 = torch.clamp(avg_x1, -50, 50)
        avg_y1 = torch.clamp(avg_y1, -50, 50)
    
        soft_x1 = F.softmax(avg_x1, dim=2).mean(1)
        soft_y1 = F.softmax(avg_y1, dim=3).mean(1)
        check_nan(soft_x1, "soft_x1")
        check_nan(soft_y1, "soft_y1")
    
        attn_map1 = torch.bmm(soft_x1, soft_y1).view(B, G, 1, H, W)
        check_nan(attn_map1, "attn_map1")
    
        # 3. Cross-Spatial Learning (Path 2)
        csl_feat = self.conv_3x3_csl(x_grouped)
        check_nan(csl_feat, "csl_feat")
    
        avg_x2 = F.adaptive_avg_pool2d(csl_feat, (H, 1))
        avg_y2 = F.adaptive_avg_pool2d(csl_feat, (1, W))
        avg_x2 = torch.clamp(avg_x2, -50, 50)
        avg_y2 = torch.clamp(avg_y2, -50, 50)
    
        soft_x2 = F.softmax(avg_x2, dim=2).mean(1)
        soft_y2 = F.softmax(avg_y2, dim=3).mean(1)
        check_nan(soft_x2, "soft_x2")
        check_nan(soft_y2, "soft_y2")
    
        attn_map2 = torch.bmm(soft_x2, soft_y2).view(B, G, 1, H, W)
        check_nan(attn_map2, "attn_map2")
    
        # Combine and output
        attn_sigmoid = self.sigmoid(attn_map1 + attn_map2)
        check_nan(attn_sigmoid, "attn_sigmoid")
    
        residual = attn_sigmoid + x_grouped.view(B, G, gc, H, W)
        check_nan(residual, "residual")
    
        out = x_reweighted.view(B, G, gc, H, W) * residual
        out = out.view(B, C, H, W)
        check_nan(out, "output out")
    
        return out

class C2f_EMA(nn.Module):
    def __init__(self, c1, c2, num_blocks=3, shortcut=False, g=1, e=0.5):
        super().__init__()
        c_ = int(c2 * e)

        # Initial projection and EMA
        self.cv1 = Conv(c1, 2 * c_, 1, 1)
        self.ema = EMA(2 * c_)

        # First bottleneck after EMA
        self.m1 = Bottleneck(2 * c_, c_, shortcut, g, e=1.0)

        # Split path after EMA
        self.cv2 = Conv(2 * c_, c_, 1, 1)

        # Second and third bottlenecks
        self.m2 = Bottleneck(c_, c_, shortcut, g, e=1.0)
        self.m3 = Bottleneck(c_, c_, shortcut, g, e=1.0)

        # üî• Final output projection
        self.cv3 = Conv(5 * c_, c2, 1, 1)  # <-- FIXED from 4c_ to 5c_

    def forward(self, x):
        if torch.isnan(x).any():
          print(f"[DEBUG] x output contains NaNs.")
        y = self.cv1(x)
        if torch.isnan(y).any():
          print(f"[DEBUG] y output contains NaNs.")
        y = self.ema(y)
        if torch.isnan(y).any():
          print(f"[DEBUG] y ema output contains NaNs.")
        y1 = self.m1(y)
        if torch.isnan(x).any():
          print(f"[DEBUG] y1 output contains NaNs.")
        y2 = self.cv2(y)
        if torch.isnan(x).any():
          print(f"[DEBUG] y2 output contains NaNs.")
        y2a = self.m2(y2)
        if torch.isnan(x).any():
          print(f"[DEBUG] y2a output contains NaNs.")
        y2b = self.m3(y2a)
        if torch.isnan(x).any():
          print(f"[DEBUG] y2b output contains NaNs.")
        # Concatenate everything
        out = self.cv3(torch.cat((y, y1, y2a, y2b), dim=1))
        #print(out.shape)
        return out
