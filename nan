class EMA(nn.Module):
    def __init__(self, channels, groups=4):
        super(EMA, self).__init__()
        assert channels % groups == 0, "Channels must be divisible by groups"
        self.groups = groups
        self.gc = channels // groups

        self.fusion_conv = nn.Conv2d(self.gc, self.gc, kernel_size=1, bias=False)
        self.conv_3x3_csl = nn.Conv2d(self.gc, self.gc, kernel_size=3, padding=1, bias=False)
        self.norm = nn.GroupNorm(groups, channels)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        # Force float32 for numerical stability
        x = x.float()
        B, C, H, W = x.shape
        G, gc = self.groups, self.gc

        x_grouped = x.view(B, G, gc, H, W).reshape(B * G, gc, H, W)

        # 1. Directional Attention
        x_pool = F.adaptive_avg_pool2d(x_grouped, (1, W))
        y_pool = F.adaptive_avg_pool2d(x_grouped, (H, 1)).permute(0, 1, 3, 2)
        fusion = torch.cat([x_pool, y_pool], dim=3)
        fusion = self.fusion_conv(fusion)

        weight_x = self.sigmoid(fusion[:, :, 0, :W].unsqueeze(2))
        weight_y = self.sigmoid(fusion[:, :, 0, W:].unsqueeze(3))

        # Clamp to avoid 0 or extreme values
        weight_x = torch.clamp(weight_x, 1e-6, 1.0)
        weight_y = torch.clamp(weight_y, 1e-6, 1.0)
        directional_attn = torch.matmul(weight_y, weight_x)
        directional_attn = torch.clamp(directional_attn, 0, 1e4)

        x_reweighted = x_grouped * directional_attn
        x_reweighted = torch.clamp(x_reweighted, -1e4, 1e4)

        # 2. Cross-Spatial Learning (Path 1)
        normed_input = x_reweighted.view(B, G, gc, H, W).view(B, C, H, W)
        normed = self.norm(normed_input).view(B * G, gc, H, W)

        avg_x1 = torch.clamp(F.adaptive_avg_pool2d(normed, (H, 1)), -50, 50)
        avg_y1 = torch.clamp(F.adaptive_avg_pool2d(normed, (1, W)), -50, 50)
        soft_x1 = F.softmax(avg_x1, dim=2).mean(1)
        soft_y1 = F.softmax(avg_y1, dim=3).mean(1)
        attn_map1 = torch.bmm(soft_x1, soft_y1).view(B, G, 1, H, W)

        # 3. Cross-Spatial Learning (Path 2)
        csl_feat = self.conv_3x3_csl(x_grouped)
        avg_x2 = torch.clamp(F.adaptive_avg_pool2d(csl_feat, (H, 1)), -50, 50)
        avg_y2 = torch.clamp(F.adaptive_avg_pool2d(csl_feat, (1, W)), -50, 50)
        soft_x2 = F.softmax(avg_x2, dim=2).mean(1)
        soft_y2 = F.softmax(avg_y2, dim=3).mean(1)
        attn_map2 = torch.bmm(soft_x2, soft_y2).view(B, G, 1, H, W)

        # Combine and residual
        attn_sigmoid = self.sigmoid(attn_map1 + attn_map2)
        residual = attn_sigmoid + x_grouped.view(B, G, gc, H, W)
        residual = torch.clamp(residual, -1e4, 1e4)

        out = x_reweighted.view(B, G, gc, H, W) * residual
        out = torch.clamp(out.view(B, C, H, W), -1e4, 1e4)

        # Debug prints for NaNs
        if torch.isnan(out).any():
            print("[DEBUG] NaNs detected in EMA output!")
            print(f"Min: {torch.nanmin(out)}, Max: {torch.nanmax(out)}")

        return out

class C2f_EMA(nn.Module):
    def __init__(self, c1, c2, num_blocks=3, shortcut=False, g=1, e=0.5):
        super().__init__()
        c_ = int(c2 * e)

        # Initial projection and EMA
        self.cv1 = Conv(c1, 2 * c_, 1, 1)
        self.ema = EMA(2 * c_)

        # First bottleneck after EMA
        self.m1 = Bottleneck(2 * c_, c_, shortcut, g, e=1.0)

        # Split path after EMA
        self.cv2 = Conv(2 * c_, c_, 1, 1)

        # Second and third bottlenecks
        self.m2 = Bottleneck(c_, c_, shortcut, g, e=1.0)
        self.m3 = Bottleneck(c_, c_, shortcut, g, e=1.0)

        # Final output projection
        self.cv3 = Conv(5 * c_, c2, 1, 1)

    def forward(self, x):
        x = x.float()  # stability
        # Step 1: initial projection
        y = self.cv1(x)
        y = torch.clamp(y, -1e4, 1e4)
        if torch.isnan(y).any():
            print("[DEBUG] NaNs after cv1!")

        # Step 2: EMA block
        y = self.ema(y)
        if torch.isnan(y).any():
            print("[DEBUG] NaNs after EMA!")

        # Step 3: first bottleneck
        y1 = self.m1(y)
        y1 = torch.clamp(y1, -1e4, 1e4)
        if torch.isnan(y1).any():
            print("[DEBUG] NaNs after m1!")

        # Step 4: second path
        y2 = self.cv2(y)
        y2 = torch.clamp(y2, -1e4, 1e4)
        if torch.isnan(y2).any():
            print("[DEBUG] NaNs after cv2!")

        y2a = self.m2(y2)
        y2a = torch.clamp(y2a, -1e4, 1e4)
        if torch.isnan(y2a).any():
            print("[DEBUG] NaNs after m2!")

        y2b = self.m3(y2a)
        y2b = torch.clamp(y2b, -1e4, 1e4)
        if torch.isnan(y2b).any():
            print("[DEBUG] NaNs after m3!")

        # Step 5: concatenate and final projection
        out = torch.cat((y, y1, y2a, y2b), dim=1)
        out = torch.clamp(out, -1e4, 1e4)
        out = self.cv3(out)
        out = torch.clamp(out, -1e4, 1e4)

        if torch.isnan(out).any():
            print("[DEBUG] NaNs detected in final output!")
            print(f"Min: {torch.nanmin(out)}, Max: {torch.nanmax(out)}")

        return out

